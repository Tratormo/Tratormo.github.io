---
---
@article{shevchenko2022fundamental,
  abstract={Autoencoders are a popular model in many branches of machine learning and lossy data compression. However, their fundamental limits, the performance of gradient methods and the features learnt during optimization remain poorly understood, even in the two-layer setting. In fact, earlier work has considered either linear autoencoders or specific training regimes (leading to vanishing or diverging compression rates). Our paper addresses this gap by focusing on non-linear two-layer autoencoders trained in the challenging proportional regime in which the input dimension scales linearly with the size of the representation. Our results characterize the minimizers of the population risk, and show that such minimizers are achieved by gradient methods; their structure is also unveiled, thus leading to a concise description of the features obtained via training. For the special case of a sign activation function, our analysis establishes the fundamental limits for the lossy compression of Gaussian sources via (shallow) autoencoders. Finally, while the results are proved for Gaussian data, numerical simulations on standard datasets display the universality of the theoretical predictions. },
  title={Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient Methods},
  author={Shevchenko*, Alexander and K{\"o}gler*, Kevin and Hassani, Hamed and Mondelli, Marco},
  journal={arXiv preprint arXiv:2212.13468},
  year={2022},
  selected={true},
  abbr={arXiv},
  pdf={https://arxiv.org/pdf/2212.13468.pdf}
  }

  @inproceedings{venkataramanan2022estimation,
  abstract={We consider the problem of signal estimation in
generalized linear models defined via rotationally invariant design matrices. Since these matrices can have an arbitrary spectral distribution,
this model is well suited for capturing complex
correlation structures which often arise in applications. We propose a novel family of approximate message passing (AMP) algorithms for signal estimation, and rigorously characterize their
performance in the high-dimensional limit via a
state evolution recursion. Our rotationally invariant AMP has complexity of the same order as
the existing AMP derived under the restrictive
assumption of a Gaussian design; our algorithm
also recovers this existing AMP as a special case.
Numerical results showcase a performance close
to Vector AMP (which is conjectured to be Bayes-
optimal in some settings), but obtained with a
much lower complexity, as the proposed algorithm does not require a computationally expensive singular value decomposition.},
  title={Estimation in rotationally invariant generalized linear models via approximate message passing},
  author={Venkataramanan, Ramji and K{\"o}gler, Kevin and Mondelli, Marco},
  booktitle={International Conference on Machine Learning},
  pages={22120--22144},
  year={2022},
  organization={PMLR},
  selected={true},
  abbr={ICML},
  pdf={https://proceedings.mlr.press/v162/venkataramanan22a/venkataramanan22a.pdf}
}
@article{kogler2021lieb,
abstract={We consider an analogue of the Lieb–Thirring inequality for quantum systems with homogeneous repulsive interaction potentials, but without the antisymmetry assumption on the wave functions. We show that in the strong-coupling limit, the Lieb–Thirring constant converges to the optimal constant of the one-body Gagliardo–Nirenberg interpolation inequality without interaction.},
  title={The Lieb--Thirring inequality for interacting systems in strong-coupling limit},
  author={K{\"o}gler, Kevin and Nam, Phan Th{\`a}nh},
  journal={Archive for Rational Mechanics and Analysis},
  volume={240},
  pages={1169--1202},
  year={2021},
  publisher={Springer},
  selected={true},
  pdf={https://link.springer.com/content/pdf/10.1007/s00205-021-01633-8.pdf?pdf=button}
}






